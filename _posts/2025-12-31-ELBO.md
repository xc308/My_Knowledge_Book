---
layout: post
title: "How to derive the Evidence Lower Bound (ELBO) for the diffusion training loss?"
math: true
date: 2025-12-31
---

To derive the Evidence Lower Bound (ELBO), we start from the training loss $L$. 
The explanation of this loss function $L$ can be found in [this post](https://xc308.github.io/What-Why-and-How-Generative-AI-Blogs/2025/12/31/Diffusion-Training-Loss.html).

$$
\begin{aligned}
L
&= \int q\!\left(x^{(0)}\right) \log p\!\left(x^{(0)}\right) \mathrm{d}x^{(0)} \\
&= \int q\!\left(x^{(0)}\right) \log\!\left[\int p\!\left(x^{(0,\ldots,T)}\right) \frac{q\!\left(x^{(1,\ldots,T)}\mid x^{(0)}\right)}{q\!\left(x^{(1,\ldots,T)} \mid x^{(0)}\right)} \mathrm{d}x^{(1,\ldots,T)} \right]\mathrm{d}x^{(0)} \\
&= \int q\!\left(x^{(0)}\right) \log\!\left[
\int q\!\left(x^{(1,\ldots,T)}\mid x^{(0)}\right)
\frac{p\!\left(x^{(0,\ldots,T)}\right)}{q\!\left(x^{(1,\ldots,T)}\mid x^{(0)}\right)}
\mathrm{d}x^{(1,\ldots,T)}
\right]\mathrm{d}x^{(0)}.
\end{aligned}
$$


In which, 
$$
q(x^{(1, \ldots, T)}| x^{(0)}) \times q(x^{(0)}) = q(x^{(0, \ldots, T)}) = q(x^{(0)}) \times \Pi_{t=1}^{T} q(x^{t} | x^{(t-1)}), \\
$$
therefore, 

$$
q(x^{(1, \ldots, T)}| x^{(0)}) = \Pi_{t=1}^{T} q(x^{t} | x^{(t-1)}).
$$

Similarly, 

$$
p(x^{0, \ldots, T}) = p(x^{T}) \! \Pi_{t = 1}^T p(x^{t-1} \mid x(t))
$$


