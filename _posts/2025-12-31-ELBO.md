---
layout: post
title: "How to derive the Evidence Lower Bound (ELBO) for the diffusion training loss?"
math: true
date: 2025-12-31
---

To derive the Evidence Lower Bound (ELBO), we start from the training loss $L$. 
The explanation of this loss function $L$ can be found in [this post](https://xc308.github.io/What-Why-and-How-Generative-AI-Blogs/2025/12/31/Diffusion-Training-Loss.html).

$$
\begin{aligned}
L &= \int dx^{(0)} q(x^{(0)}) log^{p(x^{(0)})} \\
  &= \int dx^{(0)} q(x^{(0)}) log \[\int dx^{(1, \ldots, T)} p(x^{(0, \ldots, T)}) \frac{q(x^{(1, \ldots, T)}| x^{(0)})}{q(x^{(1, \ldots, T)}| x^{(0)})} \] \\
  &= \int dx^{(0)} q(x^{(0)}) log \[\int dx^{(1, \ldots, T)} q(x^{(1, \ldots, T)}| x^{(0)}) \frac{p(x^{(0, \ldots, T)})}{q(x^{(1, \ldots, T)}| x^{(0)})} \]
\end{aligned}
$$


In which, 
$$
\begin{aligned}
q(x^{(1, \ldots, T)}| x^{(0)}) \times q(x^{(0)}) &= q(x^{(0, \ldots, T)}) = q(x^{(0)}) \times \pi_{t=1}^{T} q(x^{t} | x^{(t-1)}),  \\
q(x^{(1, \ldots, T)}| x^{(0)}) &= \Pi_{t=1}^{T} q(x^{t} | x^{(t-1)})
\end{aligned}
